{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+L2K6j3Qpad2vEagxbc5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/DataAnonymPipeline/blob/main/II_DataAnonymizationVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fYrW7di8glIt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/desstaw/Seminar_DataManagement23/main/datasets/heart.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The function make_encoder creates an encoder neural network for the Variational Autoencoder. The encoder network takes in input data with a specified shape (input_shape) and encodes it into a lower-dimensional representation called the latent space, which has a specified number of dimensions (latent_dim).*\n",
        "\n",
        "### First: Encode\n",
        "\n",
        "####A breakdown of each line in the function:\n",
        "\n",
        "`inputs = layers.Input(shape=input_shape)`: This creates an input layer for the neural network that matches the specified input_shape.\n",
        "\n",
        "`x = layers.Dense(128, activation='relu')(inputs`): This creates a fully connected (dense) layer with 128 neurons, which takes in the input data and applies the ReLU activation function to its output.\n",
        "\n",
        "`x = layers.Dense(64, activation='relu')(x)`: This creates another dense layer with 64 neurons, which takes in the output from the previous layer and applies the ReLU activation function to its output.\n",
        "\n",
        "`z_mean = layers.Dense(latent_dim)(x)`: This creates the mean layer for the latent space representation, which is another dense layer with latent_dim neurons.\n",
        "\n",
        "`z_log_var = layers.Dense(latent_dim)(x)`: This creates the log variance layer for the latent space representation, which is also a dense layer with latent_dim neurons.\n",
        "\n",
        "`return models.Model(inputs, [z_mean, z_log_var])`: This creates a Keras Model object with the inputs layer as the input and `[z_mean, z_log_var]` as the output."
      ],
      "metadata": {
        "id": "l36Ycq2PjG4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **\"latent_dim\"** is the dimensionality of the latent space, which is the space in which the input data is encoded into a compressed representation by the encoder network. In other words, it is the number of hidden variables that are used to represent the input data in a lower-dimensional space.\n",
        "\n",
        "The **\"input_shape\"** is defined by the shape of the input data. In the case of this code, the input data is a pandas DataFrame, and the \"input_shape\" is defined as the number of columns in the DataFrame, which represents the number of features that are used to describe each data point. The input shape is used to define the shape of the input layer of the encoder network, so that the network knows how to process the input data.\n",
        "\n",
        "The **\"latent space\"** is a lower-dimensional representation of the input data that is learned by the encoder network. This representation is then used by the decoder network to generate new data. By learning a compressed, lower-dimensional representation of the input data, the VAE can capture the underlying structure and patterns in the data, making it more efficient to generate new data points in the latent space rather than in the high-dimensional input space."
      ],
      "metadata": {
        "id": "_EsDBG7aj_09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TO2PYqzuo9xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the encoder network\n",
        "def make_encoder(input_shape, latent_dim):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(128, activation='relu')(inputs)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "    return models.Model(inputs, [z_mean, z_log_var])\n",
        "\n"
      ],
      "metadata": {
        "id": "EE4hQ98hiUSN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second: Decode\n",
        "The decoder network is the second part of the variational autoencoder that takes the low-dimensional encoded data from the encoder and reconstructs the original high-dimensional data. In this code, the `make_decoder` function is defined to create the decoder network.\n",
        "\n",
        "The `latent_dim` parameter that we defined earlier represents the number of dimensions in the low-dimensional space to which we are encoding the high-dimensional data. This same parameter is also passed into the `make_decoder` function to ensure that the input shape of the decoder matches the output shape of the encoder.\n",
        "\n",
        "First, the function takes in the `latent_dim` parameter and creates an input layer with shape `(latent_dim,)`. This input layer will take in the encoded data that was output from the encoder network. This is where the compressed latent space representation is fed into the decoder.\n",
        "\n",
        "The input is then passed through two dense layers with 64 and 128 nodes respectively, and ReLU activation functions. These layers serve as the hidden layers in the decoder network and are designed to transform the low-dimensional data into a representation that can be mapped back to the high-dimensional space. Why ReLu? for its ability to handle non-linearities.\n",
        "\n",
        "Finally, the output layer is defined with the same number of nodes as the original input data, and the activation function used is linear. This means that the output values will not be constrained to a particular range, allowing the decoder to output any value from the original input range. The decoder model is then returned using the `models.Model` function from Keras, with the input layer and output layer specified as inputs to the function."
      ],
      "metadata": {
        "id": "2fLSo_h5kpOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the decoder network\n",
        "def make_decoder(latent_dim):\n",
        "    inputs = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(X.shape[1], activation='linear')(x)\n",
        "    return models.Model(inputs, outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "qwc2JuwPiYe1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Third: Sample\n",
        "The Sampling layer takes the mean and log variance of the learned distribution of the latent space and randomly samples new points from this distribution to generate new data points.\n",
        "\n",
        "`class Sampling(layers.Layer)`: - define a new Keras layer for sampling the latent space\n",
        "\n",
        "`def call(self, inputs)`: - define the method to be called when the layer is used in a model\n",
        "\n",
        "`z_mean, z_log_var = inputs` - extract the mean and log variance of the distribution of the latent space from the input\n",
        "\n",
        "`batch = K.shape(z_mean)[0]` - get the batch size of the input data\n",
        "\n",
        "`dim = K.int_shape(z_mean)[1]` - get the dimensionality of the latent space\n",
        "\n",
        "`epsilon = K.random_normal(shape=(batch, dim))` - sample random noise from a normal distribution of the same size as the input data\n",
        "\n",
        "`return z_mean + K.exp(0.5 * z_log_var) * epsilon` - return a new point in the latent space by adding the mean to the product of the exponential of half the log variance and the random noise."
      ],
      "metadata": {
        "id": "yjaGCW89oiCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sampling layer\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "metadata": {
        "id": "oor8ZY2micNF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fourth: VAE model\n",
        "The VAE model combines the encoder, decoder, and sampling layers to learn a compressed representation of the input data.\n",
        "\n",
        "`inputs = layers.Input(shape=X.shape[1:])` defines the shape of the input data.\n",
        "z_mean, z_log_var = encoder(inputs) gets the mean and variance of the latent space from the encoder network.\n",
        "\n",
        "`z = Sampling()([z_mean, z_log_var])` applies the Sampling layer to the mean and variance of the latent space to generate a sample from the latent space.\n",
        "\n",
        "`outputs = decoder(z)` passes the sampled latent space through the decoder network to generate the reconstructed output.\n",
        "\n",
        "`reconstruction_loss = K.mean(K.square(inputs - outputs), axis=-1)` calculates the reconstruction loss, which measures how well the output matches the input.\n",
        "\n",
        "`kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)` calculates the Kullback-Leibler divergence, which measures how much information is lost when the input is compressed into the latent space.\n",
        "\n",
        "`vae_loss = K.mean(reconstruction_loss + beta * kl_loss)` combines the reconstruction and KL losses with a hyperparameter beta to get the overall VAE loss.\n",
        "\n",
        "`vae = models.Model(inputs, outputs)` defines the VAE model as a Keras model with the input and output tensors.\n",
        "\n",
        "`vae.add_loss(vae_loss)` adds the VAE loss to the model as an additional loss function to be optimized during training."
      ],
      "metadata": {
        "id": "RNcKLfhvpT_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the VAE model\n",
        "def make_vae(encoder, decoder, beta=1.0):\n",
        "    inputs = layers.Input(shape=X.shape[1:])\n",
        "    z_mean, z_log_var = encoder(inputs)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    outputs = decoder(z)\n",
        "    reconstruction_loss = K.mean(K.square(inputs - outputs), axis=-1)\n",
        "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    vae_loss = K.mean(reconstruction_loss + beta * kl_loss)\n",
        "    vae = models.Model(inputs, outputs)\n",
        "    vae.add_loss(vae_loss)\n",
        "    return vae\n"
      ],
      "metadata": {
        "id": "5seuoCMkpwz-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fifth: Execute"
      ],
      "metadata": {
        "id": "msNbTZoCp5Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dimensions of the latent space\n",
        "latent_dim = 2\n",
        "\n",
        "# Create the encoder, decoder, and VAE models\n",
        "encoder = make_encoder(X.shape[1:], latent_dim)\n",
        "decoder = make_decoder(latent_dim)\n",
        "vae = make_vae(encoder, decoder)\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE model\n",
        "vae.fit(X, epochs=100, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Encode the data and generate new, anonymous data\n",
        "z_mean, _ = encoder.predict(X)\n",
        "new_data = decoder.predict(np.random.normal(size=(X.shape[0], latent_dim)))\n",
        "new_data = scaler.inverse_transform(new_data)\n",
        "\n",
        "# Save the new data to a CSV file\n",
        "new_df = pd.DataFrame(new_data, columns=df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9pWzL79if79",
        "outputId": "f4c812b8-c72c-46fb-feb0-3e48424ed802"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 2s 15ms/step - loss: 1.0713 - val_loss: 1.0698\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 1.0073 - val_loss: 1.0742\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 1.0002 - val_loss: 1.0710\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.9981 - val_loss: 1.0780\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.9979 - val_loss: 1.0608\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9958 - val_loss: 1.0677\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9964 - val_loss: 1.0639\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9952 - val_loss: 1.0749\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9959 - val_loss: 1.0589\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9940 - val_loss: 1.0617\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9946 - val_loss: 1.0650\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9950 - val_loss: 1.0655\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9944 - val_loss: 1.0621\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9943 - val_loss: 1.0620\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9927 - val_loss: 1.0652\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9938 - val_loss: 1.0598\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9955 - val_loss: 1.0650\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9937 - val_loss: 1.0605\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.9915 - val_loss: 1.0714\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9916 - val_loss: 1.0718\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9943 - val_loss: 1.0623\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9934 - val_loss: 1.0620\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9931 - val_loss: 1.0610\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9918 - val_loss: 1.0625\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.9923 - val_loss: 1.0638\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9922 - val_loss: 1.0644\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9934 - val_loss: 1.0673\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9914 - val_loss: 1.0664\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9932 - val_loss: 1.0669\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9931 - val_loss: 1.0594\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9928 - val_loss: 1.0609\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9919 - val_loss: 1.0659\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9921 - val_loss: 1.0643\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9921 - val_loss: 1.0648\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9922 - val_loss: 1.0700\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9920 - val_loss: 1.0616\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9917 - val_loss: 1.0668\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9925 - val_loss: 1.0599\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9931 - val_loss: 1.0616\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9924 - val_loss: 1.0668\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9927 - val_loss: 1.0632\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9913 - val_loss: 1.0667\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9920 - val_loss: 1.0670\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9917 - val_loss: 1.0613\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9925 - val_loss: 1.0624\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9923 - val_loss: 1.0632\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.9914 - val_loss: 1.0652\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9922 - val_loss: 1.0575\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9927 - val_loss: 1.0620\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9919 - val_loss: 1.0600\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9905 - val_loss: 1.0625\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9926 - val_loss: 1.0627\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.9910 - val_loss: 1.0658\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.9915 - val_loss: 1.0657\n",
            "Epoch 55/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9919 - val_loss: 1.0679\n",
            "Epoch 56/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9927 - val_loss: 1.0672\n",
            "Epoch 57/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9936 - val_loss: 1.0652\n",
            "Epoch 58/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9905 - val_loss: 1.0654\n",
            "Epoch 59/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9923 - val_loss: 1.0637\n",
            "Epoch 60/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9914 - val_loss: 1.0647\n",
            "Epoch 61/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9916 - val_loss: 1.0624\n",
            "Epoch 62/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9916 - val_loss: 1.0669\n",
            "Epoch 63/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9916 - val_loss: 1.0622\n",
            "Epoch 64/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9920 - val_loss: 1.0659\n",
            "Epoch 65/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 0.9930 - val_loss: 1.0664\n",
            "Epoch 66/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.9918 - val_loss: 1.0657\n",
            "Epoch 67/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.9919 - val_loss: 1.0653\n",
            "Epoch 68/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 0.9907 - val_loss: 1.0635\n",
            "Epoch 69/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 0.9912 - val_loss: 1.0647\n",
            "Epoch 70/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 0.9924 - val_loss: 1.0611\n",
            "Epoch 71/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.9922 - val_loss: 1.0632\n",
            "Epoch 72/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9921 - val_loss: 1.0649\n",
            "Epoch 73/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9919 - val_loss: 1.0656\n",
            "Epoch 74/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9922 - val_loss: 1.0653\n",
            "Epoch 75/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.9915 - val_loss: 1.0674\n",
            "Epoch 76/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9905 - val_loss: 1.0621\n",
            "Epoch 77/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9919 - val_loss: 1.0641\n",
            "Epoch 78/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9916 - val_loss: 1.0653\n",
            "Epoch 79/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9912 - val_loss: 1.0641\n",
            "Epoch 80/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.9907 - val_loss: 1.0653\n",
            "Epoch 81/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9912 - val_loss: 1.0625\n",
            "Epoch 82/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9912 - val_loss: 1.0635\n",
            "Epoch 83/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9911 - val_loss: 1.0628\n",
            "Epoch 84/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.9909 - val_loss: 1.0644\n",
            "Epoch 85/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9904 - val_loss: 1.0667\n",
            "Epoch 86/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9924 - val_loss: 1.0655\n",
            "Epoch 87/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9907 - val_loss: 1.0657\n",
            "Epoch 88/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 0.9922 - val_loss: 1.0643\n",
            "Epoch 89/100\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.9919 - val_loss: 1.0643\n",
            "Epoch 90/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 0.9924 - val_loss: 1.0638\n",
            "Epoch 91/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9911 - val_loss: 1.0637\n",
            "Epoch 92/100\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.9918 - val_loss: 1.0641\n",
            "Epoch 93/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.9909 - val_loss: 1.0655\n",
            "Epoch 94/100\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 0.9911 - val_loss: 1.0663\n",
            "Epoch 95/100\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.9920 - val_loss: 1.0655\n",
            "Epoch 96/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.9909 - val_loss: 1.0641\n",
            "Epoch 97/100\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.9910 - val_loss: 1.0644\n",
            "Epoch 98/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9917 - val_loss: 1.0650\n",
            "Epoch 99/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9915 - val_loss: 1.0644\n",
            "Epoch 100/100\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9912 - val_loss: 1.0654\n",
            "33/33 [==============================] - 0s 1ms/step\n",
            "33/33 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHs5Cds-g53G",
        "outputId": "b6adb7d3-7b7e-4ae7-f4ed-c63bd2e7bc1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1025 entries, 0 to 1024\n",
            "Data columns (total 15 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  1025 non-null   float32\n",
            " 1   age         1025 non-null   float32\n",
            " 2   sex         1025 non-null   float32\n",
            " 3   cp          1025 non-null   float32\n",
            " 4   trestbps    1025 non-null   float32\n",
            " 5   chol        1025 non-null   float32\n",
            " 6   fbs         1025 non-null   float32\n",
            " 7   restecg     1025 non-null   float32\n",
            " 8   thalach     1025 non-null   float32\n",
            " 9   exang       1025 non-null   float32\n",
            " 10  oldpeak     1025 non-null   float32\n",
            " 11  slope       1025 non-null   float32\n",
            " 12  ca          1025 non-null   float32\n",
            " 13  thal        1025 non-null   float32\n",
            " 14  target      1025 non-null   float32\n",
            "dtypes: float32(15)\n",
            "memory usage: 60.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "GSVN9QLVhBbN",
        "outputId": "884db8ce-24d3-40d6-8f2d-c25fab91573d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0        age       sex        cp    trestbps        chol  \\\n",
              "0  460.029755  54.451252  0.698240  0.962146  131.848846  246.713257   \n",
              "1  456.681793  54.470539  0.692738  0.964454  131.966736  246.868759   \n",
              "2  452.670624  54.421345  0.683312  0.961687  132.111099  246.999451   \n",
              "3  462.284302  54.499207  0.691673  0.969326  131.896927  245.124405   \n",
              "4  458.265472  54.465622  0.680545  0.967206  132.059708  245.732147   \n",
              "5  457.900940  54.450405  0.694098  0.963276  131.900864  246.603790   \n",
              "6  464.172302  54.414387  0.698970  0.957538  131.806870  246.318756   \n",
              "7  459.606049  54.393829  0.695242  0.961261  131.769897  246.508377   \n",
              "8  461.419983  54.448143  0.698043  0.959627  131.822723  246.602768   \n",
              "9  462.451477  54.382946  0.699124  0.959325  131.799332  246.689728   \n",
              "\n",
              "        fbs   restecg     thalach     exang   oldpeak     slope        ca  \\\n",
              "0  0.149817  0.513378  148.952881  0.338499  1.087283  1.385930  0.749259   \n",
              "1  0.151549  0.516347  148.908264  0.338098  1.085373  1.388475  0.751461   \n",
              "2  0.155819  0.521520  148.912186  0.338436  1.087829  1.393505  0.762546   \n",
              "3  0.148044  0.510949  148.851456  0.335850  1.082627  1.381604  0.754386   \n",
              "4  0.148039  0.527088  148.822678  0.333296  1.087184  1.386384  0.755247   \n",
              "5  0.150277  0.515062  148.960678  0.337990  1.078507  1.384395  0.750936   \n",
              "6  0.149754  0.514418  148.946640  0.339153  1.086369  1.383556  0.749700   \n",
              "7  0.148725  0.518543  149.028931  0.338974  1.079389  1.383984  0.749446   \n",
              "8  0.149997  0.514159  148.967850  0.338263  1.087912  1.385875  0.751017   \n",
              "9  0.149475  0.518075  148.993652  0.340402  1.090774  1.384789  0.746561   \n",
              "\n",
              "       thal    target  \n",
              "0  2.316273  0.513445  \n",
              "1  2.310172  0.515827  \n",
              "2  2.300417  0.514500  \n",
              "3  2.311039  0.503752  \n",
              "4  2.304846  0.500989  \n",
              "5  2.309982  0.511529  \n",
              "6  2.319414  0.511601  \n",
              "7  2.308835  0.507876  \n",
              "8  2.316669  0.512908  \n",
              "9  2.316405  0.512106  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-686ecd05-ea64-4f66-aa5a-9d58448717fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>460.029755</td>\n",
              "      <td>54.451252</td>\n",
              "      <td>0.698240</td>\n",
              "      <td>0.962146</td>\n",
              "      <td>131.848846</td>\n",
              "      <td>246.713257</td>\n",
              "      <td>0.149817</td>\n",
              "      <td>0.513378</td>\n",
              "      <td>148.952881</td>\n",
              "      <td>0.338499</td>\n",
              "      <td>1.087283</td>\n",
              "      <td>1.385930</td>\n",
              "      <td>0.749259</td>\n",
              "      <td>2.316273</td>\n",
              "      <td>0.513445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>456.681793</td>\n",
              "      <td>54.470539</td>\n",
              "      <td>0.692738</td>\n",
              "      <td>0.964454</td>\n",
              "      <td>131.966736</td>\n",
              "      <td>246.868759</td>\n",
              "      <td>0.151549</td>\n",
              "      <td>0.516347</td>\n",
              "      <td>148.908264</td>\n",
              "      <td>0.338098</td>\n",
              "      <td>1.085373</td>\n",
              "      <td>1.388475</td>\n",
              "      <td>0.751461</td>\n",
              "      <td>2.310172</td>\n",
              "      <td>0.515827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>452.670624</td>\n",
              "      <td>54.421345</td>\n",
              "      <td>0.683312</td>\n",
              "      <td>0.961687</td>\n",
              "      <td>132.111099</td>\n",
              "      <td>246.999451</td>\n",
              "      <td>0.155819</td>\n",
              "      <td>0.521520</td>\n",
              "      <td>148.912186</td>\n",
              "      <td>0.338436</td>\n",
              "      <td>1.087829</td>\n",
              "      <td>1.393505</td>\n",
              "      <td>0.762546</td>\n",
              "      <td>2.300417</td>\n",
              "      <td>0.514500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>462.284302</td>\n",
              "      <td>54.499207</td>\n",
              "      <td>0.691673</td>\n",
              "      <td>0.969326</td>\n",
              "      <td>131.896927</td>\n",
              "      <td>245.124405</td>\n",
              "      <td>0.148044</td>\n",
              "      <td>0.510949</td>\n",
              "      <td>148.851456</td>\n",
              "      <td>0.335850</td>\n",
              "      <td>1.082627</td>\n",
              "      <td>1.381604</td>\n",
              "      <td>0.754386</td>\n",
              "      <td>2.311039</td>\n",
              "      <td>0.503752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>458.265472</td>\n",
              "      <td>54.465622</td>\n",
              "      <td>0.680545</td>\n",
              "      <td>0.967206</td>\n",
              "      <td>132.059708</td>\n",
              "      <td>245.732147</td>\n",
              "      <td>0.148039</td>\n",
              "      <td>0.527088</td>\n",
              "      <td>148.822678</td>\n",
              "      <td>0.333296</td>\n",
              "      <td>1.087184</td>\n",
              "      <td>1.386384</td>\n",
              "      <td>0.755247</td>\n",
              "      <td>2.304846</td>\n",
              "      <td>0.500989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>457.900940</td>\n",
              "      <td>54.450405</td>\n",
              "      <td>0.694098</td>\n",
              "      <td>0.963276</td>\n",
              "      <td>131.900864</td>\n",
              "      <td>246.603790</td>\n",
              "      <td>0.150277</td>\n",
              "      <td>0.515062</td>\n",
              "      <td>148.960678</td>\n",
              "      <td>0.337990</td>\n",
              "      <td>1.078507</td>\n",
              "      <td>1.384395</td>\n",
              "      <td>0.750936</td>\n",
              "      <td>2.309982</td>\n",
              "      <td>0.511529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>464.172302</td>\n",
              "      <td>54.414387</td>\n",
              "      <td>0.698970</td>\n",
              "      <td>0.957538</td>\n",
              "      <td>131.806870</td>\n",
              "      <td>246.318756</td>\n",
              "      <td>0.149754</td>\n",
              "      <td>0.514418</td>\n",
              "      <td>148.946640</td>\n",
              "      <td>0.339153</td>\n",
              "      <td>1.086369</td>\n",
              "      <td>1.383556</td>\n",
              "      <td>0.749700</td>\n",
              "      <td>2.319414</td>\n",
              "      <td>0.511601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>459.606049</td>\n",
              "      <td>54.393829</td>\n",
              "      <td>0.695242</td>\n",
              "      <td>0.961261</td>\n",
              "      <td>131.769897</td>\n",
              "      <td>246.508377</td>\n",
              "      <td>0.148725</td>\n",
              "      <td>0.518543</td>\n",
              "      <td>149.028931</td>\n",
              "      <td>0.338974</td>\n",
              "      <td>1.079389</td>\n",
              "      <td>1.383984</td>\n",
              "      <td>0.749446</td>\n",
              "      <td>2.308835</td>\n",
              "      <td>0.507876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>461.419983</td>\n",
              "      <td>54.448143</td>\n",
              "      <td>0.698043</td>\n",
              "      <td>0.959627</td>\n",
              "      <td>131.822723</td>\n",
              "      <td>246.602768</td>\n",
              "      <td>0.149997</td>\n",
              "      <td>0.514159</td>\n",
              "      <td>148.967850</td>\n",
              "      <td>0.338263</td>\n",
              "      <td>1.087912</td>\n",
              "      <td>1.385875</td>\n",
              "      <td>0.751017</td>\n",
              "      <td>2.316669</td>\n",
              "      <td>0.512908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>462.451477</td>\n",
              "      <td>54.382946</td>\n",
              "      <td>0.699124</td>\n",
              "      <td>0.959325</td>\n",
              "      <td>131.799332</td>\n",
              "      <td>246.689728</td>\n",
              "      <td>0.149475</td>\n",
              "      <td>0.518075</td>\n",
              "      <td>148.993652</td>\n",
              "      <td>0.340402</td>\n",
              "      <td>1.090774</td>\n",
              "      <td>1.384789</td>\n",
              "      <td>0.746561</td>\n",
              "      <td>2.316405</td>\n",
              "      <td>0.512106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-686ecd05-ea64-4f66-aa5a-9d58448717fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-686ecd05-ea64-4f66-aa5a-9d58448717fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-686ecd05-ea64-4f66-aa5a-9d58448717fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}