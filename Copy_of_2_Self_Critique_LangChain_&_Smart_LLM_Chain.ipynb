{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dqGO7StfoTNoV606lFQoSio2xQxIlWvJ",
      "authorship_tag": "ABX9TyMGshbn+zf+eePFPaG3qZVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desstaw/DataAnonymPipeline/blob/main/Copy_of_2_Self_Critique_LangChain_%26_Smart_LLM_Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source:\n",
        "https://cobusgreyling.medium.com/self-critique-llm-chain-using-langchain-smartllmchain-d67c42a4fa83\n",
        "https://alphasec.io/langchain-decoded-part-3-prompts/\n",
        "https://api.python.langchain.com/en/latest/schema/langchain.schema.prompt_template.BasePromptTemplate.html#langchain.schema.prompt_template.BasePromptTemplate\n",
        "https://www.geeksforgeeks.org/python-convert-string-dictionary-to-dictionary/\n",
        "https://medium.com/@iryna230520/first-steps-in-langchain-the-ultimate-guide-for-beginners-part-1-2baf5a4e1b81\n",
        "https://www.packtpub.com/article-hub/building-powerful-language-models-with-prompt-engineering-and-langchain\n",
        "https://lancerninja.com/mastering-prompt-templates-with-langchain/\n",
        "https://medium.com/@princekrampah/prompt-templates-in-langchain-248c015be3e0\n",
        "https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/"
      ],
      "metadata": {
        "id": "s3za57YAZwnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain\n",
        "!pip install --quiet openai python-dotenv\n",
        "!pip install openai\n",
        "!pip install sentence-transformers\n",
        "#!pip install \"langchain[docarray]\"\n",
        "!pip install langchain_experimental\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLOPACBM0iIP",
        "outputId": "1c96ac9f-4d5b-4ba3-e2ac-31f5ccebf02b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=7b4e6fb7e09a4d70509bee0b6f646f7962a7bd38dee903b38594f58dd068ed12\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.37-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=0.0.308 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.0.330)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (0.0.57)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.308->langchain_experimental) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.308->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.308->langchain_experimental) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.308->langchain_experimental) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.308->langchain_experimental) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.308->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.308->langchain_experimental) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.308->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.308->langchain_experimental) (1.0.0)\n",
            "Installing collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KG8fEi6sYfHM"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load environment variables"
      ],
      "metadata": {
        "id": "pJjn0A4TY21h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "_ = load_dotenv(find_dotenv())\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "PZRlRc4DY10b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia article example with different models in each step of the chain"
      ],
      "metadata": {
        "id": "LpfrA-Ww7I-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikipedia-API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvX_Wzw25bmj",
        "outputId": "3a220b72-694e-4535-c68e-bdc300530c25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikipedia-API\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2023.7.22)\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_experimental.smart_llm import SmartLLMChain\n",
        "import wikipediaapi\n",
        "from langchain.llms import OpenAI\n",
        "import openai\n",
        "import time\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "# Set up embeddings and vector store\n",
        "\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia('MyWikiApp/1.0')\n",
        "\n",
        "page_py = wiki_wiki.page('Large Language Model')\n",
        "\n",
        "context = page_py.summary\n",
        "question = \"How are large language models evaluated?\"\n",
        "\n",
        "template = \"\"\"\n",
        "Context: {context}\n",
        "Based on above context answer the following question.\n",
        "Question {question}:\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['context', 'question'],)\n",
        "\n",
        "\n",
        "chain = SmartLLMChain(\n",
        "    ideation_llm=ChatOpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"),\n",
        "    critique_llm=ChatOpenAI(temperature=0.6, model_name=\"gpt-4\"),\n",
        "    resolver_llm=ChatOpenAI(temperature=0.6, model_name=\"gpt-3.5-turbo\"),\n",
        "    prompt=prompt,\n",
        "    n_ideas=3,\n",
        "    verbose=True,\n",
        "\n",
        ")\n",
        "\n",
        "chain.run({'question': question, 'context': context})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G4kNp7dA47x0",
        "outputId": "3b67ea52-dcd0-4405-eafe-a9ec2a3506fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SmartLLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Context: A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation. LLMs are artificial neural networks (mainly transformers) and are (pre-)trained using self-supervised learning and semi-supervised learning.\n",
            "As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire embodied knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Notable examples include OpenAI's GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM (used in Bard), and Meta's LLaMa, as well as BLOOM, Ernie 3.0 Titan, and Anthropic's Claude 2.\n",
            "Based on above context answer the following question.\n",
            "Question How are large language models evaluated?:\n",
            "Answer:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/_api/deprecation.py:189: LangChainPendingDeprecationWarning: The function `from_strings` will be deprecated in a future version. Use from_messages classmethod instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Idea 1:\n",
            "\u001b[36;1m\u001b[1;3m1. Evaluation of large language models involves multiple aspects, including assessing their performance on various language tasks, measuring their language understanding and generation capabilities, and evaluating their ability to generalize to different domains and contexts.\n",
            "\n",
            "2. One common evaluation approach is to use benchmark datasets that cover a wide range of language tasks such as text classification, sentiment analysis, natural language inference, question answering, machine translation, summarization, and others. Models are evaluated based on their accuracy, precision, recall, F1 score, or other task-specific metrics.\n",
            "\n",
            "3. Another aspect of evaluation is assessing the quality of generated text. This involves measuring attributes like fluency, coherence, grammaticality, and relevance to the given prompt or context. Human evaluators may be involved to provide judgments or ratings based on predefined criteria.\n",
            "\n",
            "4. Evaluation also includes analyzing the model's response to different prompts or inputs to understand its reasoning abilities and the extent to which it can handle ambiguity, context, and nuanced language understanding.\n",
            "\n",
            "5. Bias evaluation is an important aspect given the potential biases present in the training data. Evaluators check for biases in model outputs, such as biased language, stereotypes, or unfair treatment towards certain groups. Techniques like counterfactual evaluation, probing tasks, or adversarial testing may be used to identify and mitigate biases.\n",
            "\n",
            "6. Fine-tuning and transfer learning techniques are evaluated to understand the effectiveness of adapting models to specific tasks. This involves measuring the performance of the fine-tuned models on task-specific datasets and comparing it to the performance of the base model.\n",
            "\n",
            "7. In addition to quantitative evaluation metrics, qualitative analysis and user feedback play a crucial role in evaluating the usability and overall performance of large language models. User studies, surveys, and feedback from human reviewers or beta testers help assess the model's practicality and its ability to meet user needs effectively.\n",
            "\n",
            "Overall, large language models are evaluated through a combination of quantitative metrics, qualitative analysis, benchmark datasets, human evaluators, and user feedback to assess their performance, language understanding, generation capabilities, biases, and practical usability.\u001b[0m\n",
            "Idea 2:\n",
            "\u001b[36;1m\u001b[1;3m1. Large language models are evaluated using a variety of metrics and techniques to assess their performance and capabilities.\n",
            "2. One common evaluation method is to measure the model's ability to generate coherent and contextually appropriate text. This can be done by providing a prompt or a beginning of a sentence and evaluating how well the model can complete it.\n",
            "3. Evaluation can also include measuring the model's fluency, which is how well it produces text that is grammatically correct and natural-sounding.\n",
            "4. Another aspect of evaluation is assessing the model's semantic understanding. This involves testing whether the model can accurately comprehend and respond to various types of questions or queries.\n",
            "5. Large language models are also evaluated for their ability to avoid biases and generate unbiased and fair text. This is particularly important as language models tend to learn from biased training data, and efforts are made to minimize the propagation of these biases in the generated text.\n",
            "6. Additionally, human evaluators play a crucial role in evaluating large language models. They provide subjective assessments and evaluate the quality of the generated text based on their expertise in language and contextual comprehension.\n",
            "7. Other evaluation methods may involve comparing the model's outputs to a gold standard or reference text to measure the similarity or correctness of the generated text.\n",
            "8. It is also common to conduct evaluations on specific tasks or benchmarks to assess the model's performance in areas such as question answering, summarization, translation, or sentiment analysis.\n",
            "9. In some cases, large language models are tested against specific test sets or datasets specifically created for evaluation purposes. These datasets are designed to test the model's performance on specific linguistic phenomena or challenges.\n",
            "10. Overall, a combination of automated metrics, human evaluation, and specialized tests is used to evaluate the performance and capabilities of large language models.\u001b[0m\n",
            "Idea 3:\n",
            "\u001b[36;1m\u001b[1;3m1. Large language models are evaluated using a variety of metrics and benchmarks to assess their performance and capabilities.\n",
            "\n",
            "2. One commonly used evaluation metric is perplexity, which measures how well a language model can predict the next word in a sequence given the context. Lower perplexity values indicate better performance.\n",
            "\n",
            "3. Another evaluation approach is to use human evaluation, where human judges assess the quality, fluency, and coherence of generated text produced by the language model. This helps assess the model's ability to generate human-like and contextually appropriate responses.\n",
            "\n",
            "4. In addition to perplexity and human evaluation, large language models may be evaluated on specific tasks by comparing their performance with benchmark datasets. For example, models can be evaluated on their ability to perform question answering, text completion, sentiment analysis, translation, summarization, or dialogue generation tasks.\n",
            "\n",
            "5. Some evaluation frameworks, such as GLUE (General Language Understanding Evaluation) and SuperGLUE, provide standardized benchmarks that cover a range of natural language understanding and generation tasks. These benchmarks facilitate fair comparison and evaluation of different language models.\n",
            "\n",
            "6. Evaluating large language models also involves assessing their robustness to adversarial attacks, bias in responses, and ethical considerations. Models should be evaluated for their ability to handle different contexts, understand nuanced language, avoid generating offensive or biased content, and maintain privacy and data protection.\n",
            "\n",
            "7. It is important to note that evaluating large language models is an ongoing process, and researchers continually refine and update evaluation methods to ensure accurate and comprehensive assessment of the models' capabilities and limitations.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/_api/deprecation.py:189: LangChainPendingDeprecationWarning: The function `from_strings` will be deprecated in a future version. Use from_messages classmethod instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Critique:\n",
            "\u001b[33;1m\u001b[1;3mOption 1:\n",
            "1. The answer assumes that large language models can be evaluated effectively using a wide variety of methods, but it doesn't take into account the inherent challenges in evaluating such complex models, such as the difficulty in assessing the model's understanding of nuanced language or the lack of standardized benchmarks for certain tasks.\n",
            "2. The answer mentions the use of human evaluators for assessing the quality of generated text, but it doesn't address the potential issues with this approach, such as subjectivity and inconsistency in human judgments.\n",
            "3. The discussion of bias evaluation is somewhat simplistic. While identifying and mitigating biases is crucial, the process is complex and challenging, and the answer doesn't fully convey this complexity.\n",
            "4. The answer assumes that user feedback and qualitative analysis can provide a comprehensive understanding of the model's performance, but it doesn't consider that these assessments may be influenced by individual biases and perceptions.\n",
            "\n",
            "Option 2:\n",
            "1. The answer assumes that providing a prompt or beginning of a sentence is a sufficient way to evaluate how well a model can generate text, but this may not fully test the model's ability to understand and generate text in more complex or ambiguous contexts.\n",
            "2. The mention of testing for biases in the model's output is important, but the answer doesn't address how these biases can be identified and mitigated.\n",
            "3. The answer mentions the use of a gold standard or reference text for evaluation, but it doesn't consider the difficulty in establishing such a standard for many language tasks.\n",
            "4. The answer suggests that specific test sets or datasets can effectively evaluate the model's performance, but it doesn't acknowledge the limitations of this approach, such as the risk of overfitting to the test set.\n",
            "\n",
            "Option 3:\n",
            "1. The answer mentions perplexity as a common evaluation metric, but it fails to acknowledge its limitations. Perplexity measures uncertainty and might not fully reflect the model's ability to generate meaningful and coherent text.\n",
            "2. The answer suggests that human evaluation can effectively assess the quality of generated text, but it doesn't address the issues associated with this approach, such as subjectivity and variability in human judgments.\n",
            "3. The use of standardized benchmarks like GLUE and SuperGLUE is mentioned, but the answer doesn't consider the limitations of these benchmarks, such as their focus on specific tasks and the risk of models overfitting to these benchmarks.\n",
            "4. The answer discusses the assessment of robustness to adversarial attacks, but it doesn't consider the difficulty in creating effective adversarial examples for large language models.\n",
            "5. The answer suggests that the evaluation of large language models is an ongoing process, but it doesn't acknowledge the need for more innovative and comprehensive evaluation methods to fully understand these complex models.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/_api/deprecation.py:189: LangChainPendingDeprecationWarning: The function `from_strings` will be deprecated in a future version. Use from_messages classmethod instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolution:\n",
            "\u001b[32;1m\u001b[1;3mThe improved and full answer is as follows:\n",
            "\n",
            "Answer: \n",
            "\n",
            "Large language models are evaluated using a variety of metrics, benchmarks, and techniques to assess their performance and capabilities. Evaluation involves multiple aspects, including assessing the model's performance on various language tasks, measuring its language understanding and generation capabilities, and evaluating its ability to generalize to different domains and contexts.\n",
            "\n",
            "One common evaluation method is to use benchmark datasets that cover a wide range of language tasks such as text classification, sentiment analysis, natural language inference, question answering, machine translation, summarization, and others. Models are evaluated based on task-specific metrics like accuracy, precision, recall, F1 score, or other appropriate measures.\n",
            "\n",
            "In addition to quantitative metrics, qualitative analysis and human evaluation play a crucial role in evaluating large language models. Human evaluators assess the quality, fluency, coherence, and relevance of the generated text. Their expertise in language and contextual comprehension helps determine the model's ability to produce human-like and contextually appropriate responses.\n",
            "\n",
            "Evaluating large language models also involves assessing their ability to avoid biases and generate unbiased and fair text. Techniques such as counterfactual evaluation, probing tasks, or adversarial testing may be used to identify and mitigate biases in the model's outputs.\n",
            "\n",
            "The evaluation process includes analyzing the model's response to different prompts or inputs to understand its reasoning abilities and its handling of ambiguity, context, and nuanced language understanding.\n",
            "\n",
            "It is important to consider the limitations and challenges in evaluating large language models. These models are complex and evaluating their performance comprehensively can be difficult. Researchers continually refine and update evaluation methods to ensure accurate and comprehensive assessment of the models' capabilities and limitations.\n",
            "\n",
            "Overall, the evaluation of large language models requires a combination of quantitative metrics, qualitative analysis, human evaluation, benchmark datasets, and specialized tests. This multi-faceted approach helps assess the models' performance, language understanding, generation capabilities, biases, and practical usability.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The improved and full answer is as follows:\\n\\nAnswer: \\n\\nLarge language models are evaluated using a variety of metrics, benchmarks, and techniques to assess their performance and capabilities. Evaluation involves multiple aspects, including assessing the model's performance on various language tasks, measuring its language understanding and generation capabilities, and evaluating its ability to generalize to different domains and contexts.\\n\\nOne common evaluation method is to use benchmark datasets that cover a wide range of language tasks such as text classification, sentiment analysis, natural language inference, question answering, machine translation, summarization, and others. Models are evaluated based on task-specific metrics like accuracy, precision, recall, F1 score, or other appropriate measures.\\n\\nIn addition to quantitative metrics, qualitative analysis and human evaluation play a crucial role in evaluating large language models. Human evaluators assess the quality, fluency, coherence, and relevance of the generated text. Their expertise in language and contextual comprehension helps determine the model's ability to produce human-like and contextually appropriate responses.\\n\\nEvaluating large language models also involves assessing their ability to avoid biases and generate unbiased and fair text. Techniques such as counterfactual evaluation, probing tasks, or adversarial testing may be used to identify and mitigate biases in the model's outputs.\\n\\nThe evaluation process includes analyzing the model's response to different prompts or inputs to understand its reasoning abilities and its handling of ambiguity, context, and nuanced language understanding.\\n\\nIt is important to consider the limitations and challenges in evaluating large language models. These models are complex and evaluating their performance comprehensively can be difficult. Researchers continually refine and update evaluation methods to ensure accurate and comprehensive assessment of the models' capabilities and limitations.\\n\\nOverall, the evaluation of large language models requires a combination of quantitative metrics, qualitative analysis, human evaluation, benchmark datasets, and specialized tests. This multi-faceted approach helps assess the models' performance, language understanding, generation capabilities, biases, and practical usability.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}